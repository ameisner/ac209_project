{
 "metadata": {
  "name": "",
  "signature": "sha256:9cbe500b35893bdd6629cfec03fb7471b70374b8de053641ca6e356a6ac294e4"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Classification Process Book"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from scipy.stats import scoreatpercentile\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.cross_validation import train_test_split\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "import sklearn.grid_search\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "import cPickle\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#1. The Problem\n",
      "\n",
      "We wish to perform text classification and regression on GameSpot video game reviews. The classification will take the form of labeling each review as either \"positive\" or \"negative\" based on its numerical score and training/optimizing various classifiers to predict \"positive\" versus \"negative\" sentiment based on review text. This classification goal was inspired by the application of Naive Bayes classification to Rotten Tomatoes movie reviews (\"fresh\" versus \"rotten\") in the Bayesian Tomatoes exercise from AC209 Fall 2013. Here we extend this prior work in several ways:\n",
      "1. We apply Naive Bayes classification to video game reviews from GameSpot rather than movie reviews from Rotten Tomatoes.\n",
      "2. We apply random forest classification and compare its results to those of Naive Bayes classification.\n",
      "3. We attempt to predict numerical review scores using k nearest neighbors regression.\n",
      "4. We apply feature-cleaning techniques such as regular expressions and stemming to the video game review corpus."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#2. Preliminaries"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The first step to running any text analysis on the reviews is to load them into numpy arrays from the JSON file we constructed while scraping.\n",
      "To do this, we have written a Python utility script called `load_reviews_scores.py`. This script offers the option of either stemming or not stemming the words in the reviews as an additional preprocessing step, via the `stem` keyword in the `load_reviews_scores` function. The following code cell prints the contents of this file:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print open('load_reviews_scores.py').read()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "# written by Aaron Meisner, 12/4/2014\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "def remove_empty_reviews(rev_list, scores):\n",
        "    l = np.array([len(r.replace(' ', '')) for r in rev_list])\n",
        "\n",
        "    return rev_list[l > 0], scores[l > 0]\n",
        "\n",
        "def load_reviews_scores(stem=False):\n",
        "    f = open('data/gamespot.json')\n",
        "    data = json.load(f)\n",
        "\n",
        "    rev_list = np.array([g['summary'] for g in data])\n",
        "    scores = np.array([g['score'] for g in data]).astype(float)\n",
        "\n",
        "    rev_list, scores = remove_empty_reviews(rev_list, scores)\n",
        "\n",
        "    u, ind_u = np.unique(rev_list, return_index=True)\n",
        "    rev_list = rev_list[ind_u]\n",
        "    scores = scores[ind_u]\n",
        "\n",
        "    if stem:\n",
        "        stemmer = PorterStemmer()\n",
        "        rev_list_stemmed = []\n",
        "        tokenizer = RegexpTokenizer(ur'\\b[a-zA-Z][a-zA-Z]+\\b')\n",
        "\n",
        "        for rev in rev_list:\n",
        "            tokens = tokenizer.tokenize(rev)\n",
        "            stemmed = \"\"\n",
        "            for token in tokens:\n",
        "                stemmed = stemmed + \" \" + stemmer.stem(token)\n",
        "            rev_list_stemmed.append(stemmed)\n",
        "        return rev_list_stemmed, scores\n",
        "    else:\n",
        "        return rev_list, scores\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##3. The Baseline"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In any machine learning task, especially classification, it is extremely important to establish a baseline accuracy for the *simplest possible model*. In our case, the simplest possible model corresponds to guessing that all reviews are positive or all reviews are negative, without any examination of the review text. If we do not choose our threshold for \"positive\" versus \"negative\" reviews carefully, we will end up with an unbalanced set of examples for which it is difficult to interpret accuracy scores. For example, in the classic fraud detection problem, 99.99% of examples could be non-fraudulent, and therefore a trivial solution can produce 99.99% nominal accuracy, depending on the accuracy metric used. In order to avoid the subtleties introduced by unbalanced examples, we will choose the boundary between \"positive\" and \"negative\" reviews which makes the examples maximally balanced. Below is code showing how we choose the boundary between positive and negative reviews."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from load_reviews_scores import load_reviews_scores\n",
      "reviews, scores = load_reviews_scores()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'The median review score is ', scoreatpercentile(scores, 50)\n",
      "print 'The fraction of reviews that have score greater than or equal to 7 is', float(np.sum(scores >=  7))/len(scores)\n",
      "print 'The fraction of reviews that have score greater than 7 is', float(np.sum(scores > 7))/len(scores)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The median review score is  7.0\n",
        "The fraction of reviews that have score greater than or equal to 7 is 0.535880708295\n",
        "The fraction of reviews that have score greater than 7 is 0.461747013471\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The above printouts show that we can get closest to an even split of the data by labeling \"positive\" reviews as those with score greater than\n",
      "or equal to seven, and \"negative\" reviews as those with score less than seven. With this choice of threshold, a trivial solution can achieve accuracy of 53.6% at best, by guessing that every review is positive."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##4. Train/Test Split"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the Bayesian Tomatoes exercise, 3000 reviews were collected in total. Here, we have nearly 12000 reviews. Thus, we have plenty of data and can choose a substantial number of reviews to be set aside completely as test examples, which are not used at any point during training or cross-validation."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'The total number of GameSpot reviews that we have is', len(reviews)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The total number of GameSpot reviews that we have is 11803\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Throughout our machine learning work, we will set aside aside 1803 of our examples as a test set, leaving 10000 examples on which to train and cross-validate our classification/regression models."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In order to perform our train/test split, we need to set up our feature matrix $X$ and target vector $Y$. As in Bayesian Tomatoes, we take features to be individual words by using the [bag of words](http://en.wikipedia.org/wiki/Bag-of-words_model) technique. It is extremely important to keep in mind that this bag of words model considers a block of text to be a random accumulation of various words, and that this model makes no attempt whatsoever to account for the order/grammatical structure in which these words are arranged. This is obviously a gigantic oversimplification, but it makes the problem at hand much more computationally tractable; to whatever extent higher-order groupings of words are taken as features, the classification becomes combinatorically more computationally demanding. In the following cell we set up the feature matrix $X$ using the bag of words model:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer = CountVectorizer()\n",
      "vectorizer.fit(reviews)\n",
      "X = vectorizer.transform(reviews)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Per the discussion above, the target vector $Y$ will simply be a vector of 1's and 0's and is easy to calculate. We will choose the labels such that 1 corresponds to a \"positive\" review, and \"0\" corresponds to a \"negative review\""
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Y = (scores >= 7).astype(int)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can check that X and Y have compatible dimensions:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print X.shape\n",
      "print Y.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(11803, 13631)\n",
        "(11803,)\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note that the second dimension of $X$ encodes the number of features. With this default vectorizer, we actually have more \n",
      "features than examples! The number of features can be adjusted with the `df_min` parameter of CountVectorizer. We will tune this `df_min` parameter later."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we can actually perform the train/test split. The train/test split in this case is somewhat subtle, because we will tune the `min_df` parameter (see below), which actually *changes the dimensions* of the feature matrix $X$. Therefore, in order to make sure we keep our test data truly separate from the training/cross-validation process, we will perform our train/test split by saving the *indices* of the reviews which will be used for training, and those which will be saved for testing:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.random.seed(seed=99)\n",
      "ind_train = np.random.choice(len(reviews), 10000, replace=False) \n",
      "ind_all = np.arange(len(reviews))\n",
      "ind_test = np.array(list(set(ind_all).difference(set(ind_train))))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Because some of the classifier training happens via auxiliary scripts outside of this notebook (due to the computational infeasibility of running everything within the notebook), it is necessary to write out these train/test indices to files which can be read in by these auxiliary scripts."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "o_train = open('data/ind_train.pkl', 'wb')\n",
      "cPickle.dump(ind_train, o_train)\n",
      "o_train.close()\n",
      "\n",
      "o_test = open('data/ind_test.pkl','wb')\n",
      "cPickle.dump(ind_test, o_test)\n",
      "o_test.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##5. Naivest Bayes"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here we try the simplest possible version of Naive Bayes classification. As demonstrated in Bayesian Tomatoes, there are two important parameters which must be tuned. The first is `min_df`, which encodes the minimum frequency with which a word must appear in the training corpus in order to become a feature of the model. This is useful in getting rid of random junk which may have been accidentally incorporated into the review text during scraping, or words which are proper nouns (e.g. game names) rather than part of the standard english vocabulary. In Bayesian tomatoes, it was found that `min_df`=0.001 is optimal, but here in our \"Naivest Bayes\" experiment, we simply use the default value, which includes all words that appear even once in the corpus as features. The other important parameter that should be tuned is called `alpha`, the additive (Laplace/Lidstone) smoothing parameter, which effectively smooths out individual features and thereby dictates how strongly the predicted positive/negative probabilities are dragged toward 0.50. Here, to establish a baseline, we adopt the default value of `alpha`=1."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf = MultinomialNB().fit(X[ind_train,:], Y[ind_train])\n",
      "print \"Training set accuracy: %0.2f%%\" % (100 * clf.score(X[ind_train,:], Y[ind_train]))\n",
      "print \"Test set accuracy: %0.2f%%\" % (100 * clf.score(X[ind_test,:], Y[ind_test]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training set accuracy: 89.27%\n",
        "Test set accuracy: 75.43%\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Thus, without being careful about cleaning or stemming our features, and without any cross-validation to tune the model parameters, we can already achieve an accuracy of $\\sim$75.4% on the test data. That's pretty good!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##6. Cross-Validation Framework"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Cross-validation will consist of running a two-dimensional grid in (`min_df`, `alpha`) to find the combination of these parameters which yields the highest cross-validation accuracy score during 10-fold cross-validation. Note that it is not possible in this situation to trivially use GridSearchSV as in Homework 3, because technically `min_df` is not a parameter of the Naive Bayes classifier, but rather it *dictates the shape and contents of the feature matrix, irrespective of the model trained on the feature matrix to predict the target*. Therefore, we have written a custom script to search for the optimal (`min_df`, `alpha`) pair for the Naive Bayes classification. This script is printed below:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print open('crossval_naive_bayes.py').read()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "#!/usr/bin/env python\n",
        "# written by Aaron Meisner, 12/5/2014\n",
        "\n",
        "import cPickle\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "import sklearn.grid_search\n",
        "from load_reviews_scores import load_reviews_scores\n",
        "\n",
        "def crossval_naive_bayes(stem=False, regex=False):\n",
        "    # load review/score data (pass stem keyword along)\n",
        "    reviews, scores = load_reviews_scores(stem=stem)\n",
        "\n",
        "    # construct target vector\n",
        "    Y = (scores >= 7)\n",
        "\n",
        "    # load train/test indices\n",
        "    ind_train = cPickle.load(file('data/ind_train.pkl'))\n",
        "\n",
        "    # make array of candidate min_df values\n",
        "    min_dfs = np.arange(0.00008, 0.01002, 0.00002)\n",
        "    # make array of candidate alpha values\n",
        "    alphas = np.array([0.005, 0.1, 0.5, 1, 1.7, 1.8, 1.9, 2, 2.1, 2.2, 2.3, 5, \n",
        "                       10])\n",
        "\n",
        "    n_min_df = len(min_dfs)\n",
        "    n_alpha = len(alphas)\n",
        "\n",
        "    # initialize 2d grids holding cross-validation results\n",
        "    grid_cv_score = np.zeros((n_min_df, n_alpha))\n",
        "    grid_score_std = np.zeros((n_min_df, n_alpha))\n",
        "    grid_min_df = np.zeros((n_min_df, n_alpha))\n",
        "    grid_alpha = np.zeros((n_min_df, n_alpha))\n",
        "\n",
        "    parameters = {'alpha' : alphas}\n",
        "\n",
        "    for i, min_df in enumerate(min_dfs):\n",
        "    #   print out current min_df parameter value\n",
        "        print i, min_df\n",
        "    #   construct feature matrix \n",
        "        vectorizer = CountVectorizer(min_df=min_df)\n",
        "        vectorizer.fit(reviews)\n",
        "        X = vectorizer.transform(reviews)\n",
        "  \n",
        "    #   restrict to the training indices and run 10-fold CV with GridSearchCV\n",
        "        clf_gs = sklearn.grid_search.GridSearchCV(MultinomialNB(), parameters,\n",
        "                                                   cv=10)\n",
        "        clf_gs.fit(X[ind_train,:], Y[ind_train])\n",
        "        clf_gs.grid_scores_\n",
        "\n",
        "        cv_means = np.array([clf_gs.grid_scores_[j][1] for j in range(n_alpha)])\n",
        "        cv_stds = np.array([np.std(clf_gs.grid_scores_[j][2]) for j in range(n_alpha)])\n",
        "    #   store information about inputs and scores for this value of min_df\n",
        "        grid_cv_score[i, :] = cv_means\n",
        "        grid_score_std[i, :] = cv_stds\n",
        "        grid_min_df[i, :] = min_df\n",
        "        grid_alpha[i, :] = alphas\n",
        "\n",
        "    # write out 2d grids to pkl file\n",
        "    o_scores = open('data/grid_cv_score.pkl', 'wb')\n",
        "    cPickle.dump(grid_cv_score, o_scores)\n",
        "    o_scores.close()\n",
        "\n",
        "    o_score_stds = open('data/grid_score_std.pkl', 'wb')\n",
        "    cPickle.dump(grid_score_std, o_score_stds)\n",
        "    o_score_stds.close()\n",
        "\n",
        "    o_alphas = open('data/grid_alpha.pkl', 'wb')\n",
        "    cPickle.dump(grid_alpha, o_alphas)\n",
        "    o_alphas.close()\n",
        "\n",
        "    o_min_dfs = open('data/grid_min_df.pkl', 'wb')\n",
        "    cPickle.dump(grid_min_df, o_min_dfs)\n",
        "    o_min_dfs.close()\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##7. Tuned Naive Bayes Classifier\n",
      "\n",
      "We have applied the above ideas and script to search a rectangular grid in (`min_df`, `alpha`), where the `min_df` values are\n",
      "[0.00008, 0.0001, 0.00012, 0.00014, ..., 0.00980, 0.01] and the `alpha` values are [0.005, 0.1, 0.5, 1, 1.7, 1.8, 1.9, 2, 2.1, 2.2, 2.3, 5, 10]. This represents 497 `min_df` values and 13 `alpha` values, for a total of 6461 parameter choice pairs evaluated. Note that the `min_df` values are evenly spaced in increments of $2\\times10^{-5}$, whereas the `alpha` values are not evenly spaced.\n",
      "\n",
      "We define the \"optimal\" choice of (`min_df`, `alpha`) to be that which maximizes the cross-validation accuracy score. A colorscale map of the cross-validation accuracy score over the grid of (`min_df`, `alpha`), computed using `crossval_naive_bayes.py`, is shown below (top left subplot). For comparison and ease of interpretation, several additional related colormap figures are shown. The top right panel shows the standard deviation of the cross-validation score at every point in the grid, to give a sense of the uncertainty on each cross-validation accuracy shown in the top panel. The lower left panel shows the grid of `min_df` values used. The lower right panel shows the the grid of `alpha` values used."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cv_scores = cPickle.load(file('data/grid_cv_score.pkl'))\n",
      "cv_scores_std = cPickle.load(file('data/grid_score_std.pkl'))\n",
      "alpha_grid = cPickle.load(file('data/grid_alpha.pkl'))\n",
      "min_df_grid = cPickle.load(file('data/grid_min_df.pkl'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# also show \"marginal\" 1D views?\n",
      "\n",
      "plt.figure(figsize=(14,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title('cross-validation accuracy score')\n",
      "plt.imshow(cv_scores, interpolation='nearest', aspect=0.02)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title('cross-validation accuracy score standard deviation')\n",
      "plt.imshow(cv_scores_std, interpolation='nearest', aspect=0.02)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title('min_df values for cross-validation grid')\n",
      "plt.imshow(min_df_grid, interpolation='nearest', aspect=0.02)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title('alpha values for cross-validation grid')\n",
      "plt.imshow(alpha_grid, interpolation='nearest', aspect=0.02)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 19,
       "text": [
        "<matplotlib.image.AxesImage at 0x10c695d90>"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAzEAAAKoCAYAAABHiHG/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
       "text": [
        "<matplotlib.figure.Figure at 0x108afd690>"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the following cell, we take the initial steps toward determining the optimal choice of (`min_df`, `alpha`) based on the cross-validation accuracy grids shown above:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"The maximum cross-validation accuracy score is \", np.max(cv_scores)\n",
      "w = np.where(cv_scores == np.max(cv_scores))\n",
      "print \"The data points at which the maximum accuracy score is achieved have alpha=\", alpha_grid[w[0],w[1]]\n",
      "print \"The data points at which the maximum accuracy score is achieved have min_df=\", min_df_grid[w[0],w[1]]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The maximum cross-validation accuracy score is  0.7663\n",
        "The data points at which the maximum accuracy score is achieved have alpha= [ 1.7  1.7  1.7  1.7]\n",
        "The data points at which the maximum accuracy score is achieved have min_df= [ 0.0001   0.00012  0.00014  0.00016]\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can see that there are actually four points in the grid tied for the best accuracy score. This can happen because only 1000 reviews are being used to calculate each cross-validation accuracy score, so the possible accuracy scores are effectively discretized. We can see that all of the best parameter choices have `alpha`=1.7, so for our \"optimized Naive Bayes classifier\" we will employ `alpha`=1.7. For the optimized Naive Bayes classifier, we will adopt a `min_df` value which is the median of the `min_df` values which are tied for the best cross-validation accuracy:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"We will adopt min_df=\", np.median(min_df_grid[w[0],w[1]]), \" for our optimized Naive Bayes classifier.\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "We will adopt min_df= 0.00013  for our optimized Naive Bayes classifier.\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We should take note of and attempt to understand this optimal value of  `min_df`. The default value of `min_df` corresponds to including every word that appears even once as a feature. With `min_df` at its default value, we have 13631 features. As shown below, with `min_df`=0.00013, we have only 7764 features. This means that the optimized Naive Bayes classifier uses approximately the most frequent half of all unique words as features."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer_opt = CountVectorizer(min_df=0.00013)\n",
      "vectorizer_opt.fit(reviews)\n",
      "print \"In the optimized classifier, the most frequent \", len(vectorizer_opt.get_feature_names()), \" words are used as features.\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "In the optimized classifier, the most frequent  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "7764  words are used as features.\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It is also interesting to compare our optimal values of `alpha` and `min_df` to those found in the Bayesian Tomatoes study of movie reviews. In Bayesian Tomatoes, (`alpha`, `min_df`)=(5, 0.001) was determined to be optimal. Aside from the fact that our analysis and Bayesian tomatoes rely upon completely different data sets, another reason we might end up with somewhat different values is that we chose to optimize the cross-validation accuracy score, whereas in Bayesian Tomatoes the log-likelihood was optimized. Using the optimal combination of (`min_df`, `alpha`) = (0.00013, 1.7) we can evaluate the *test* accuracy score, to see how much improvement has been achieved relative to simply using the default parameters."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_opt = vectorizer_opt.transform(reviews)\n",
      "clf_opt = MultinomialNB(alpha=1.7).fit(X_opt[ind_train,:], Y[ind_train])\n",
      "print \"Training set accuracy: %0.2f%%\" % (100 * clf_opt.score(X_opt[ind_train,:], Y[ind_train]))\n",
      "print \"Test set accuracy: %0.2f%%\" % (100 * clf_opt.score(X_opt[ind_test,:], Y[ind_test]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training set accuracy: 86.13%\n",
        "Test set accuracy: 75.93%\n"
       ]
      }
     ],
     "prompt_number": 50
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Notice that this test accuracy is only very slightly higher than the test accuracy, by about half a percent. This optimized optimized Naive Bayes classifier accuracy for video game reviews is very similar to that found for movie reviews in Bayesian Tomatoes, with their final \"calibrated\" classifier (74%)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###7.1 Understanding the Naive Bayes Classifier\n",
      "\n",
      "With the \"heavy lifting\" of training, cross-validating, optimizing and testing our Naive Bayes classifier out of the way, we can demonstrate some of its amusing properties and capabilities. A reasonable first question to ask about the classifier is: Which features (individual words) are the most strongly indicative of a \"positive\" or \"negative\" review. A way to phrase this more quantitatively within the Naive Bayes framework is to ask: which words have the highest and lowest values of P(positive|word)? The words with highest P(positive|word) are most strongly indicative of a positive review, and the words with lowest P(positive|word) are most strongly indicative of a negative review. The following code prints out the ten most \n",
      "strongly positive and strongly negative words:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "words = np.array(vectorizer_opt.get_feature_names())\n",
      "x = np.eye(X_opt.shape[1])\n",
      "probs = clf_opt.predict_log_proba(x)[:, 0]\n",
      "ix = np.argsort(probs)\n",
      "\n",
      "n_example=10\n",
      "\n",
      "good_words = words[ix[:n_example]]\n",
      "bad_words = words[ix[-n_example:]]\n",
      "bad_words = bad_words[::-1]\n",
      "\n",
      "good_prob = probs[ix[:n_example]]\n",
      "bad_prob = probs[ix[-n_example:]]\n",
      "bad_prob = bad_prob[::-1]\n",
      "\n",
      "print \"Best words\\t     P(positive | word)\"\n",
      "for w, p in zip(good_words, good_prob):\n",
      "    print \"%20s\" % w, \"%0.3f\" % (1 - np.exp(p))\n",
      "    \n",
      "print \"Worst words\\t     P(positive | word)\"\n",
      "for w, p in zip(bad_words, bad_prob):\n",
      "    print \"%20s\" % w, \"%0.3f\" % (1 - np.exp(p))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Best words\t     P(positive | word)\n",
        "              superb 0.956\n",
        "            terrific 0.931\n",
        "               tight 0.928\n",
        "            stunning 0.928\n",
        "             crafted 0.925\n",
        "               notch 0.904\n",
        "         refinements 0.904\n",
        "                maps 0.904\n",
        "        successfully 0.902\n",
        "           excellent 0.900\n",
        "Worst words\t     P(positive | word)\n",
        "            terrible 0.036\n",
        "                dull 0.063\n",
        "               avoid 0.065\n",
        "          mediocrity 0.073\n",
        "              boring 0.074\n",
        "              poorly 0.076\n",
        "                mess 0.081\n",
        "       uninteresting 0.081\n",
        "              shoddy 0.085\n",
        "                rent 0.085\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "These best/worst words are almost entirely self explanatory. For example, it would be unlikely to use the word \"superb\" in a negative review or to use the word \"mediocrity\" in a positive review. A few of these words are worthy of further exploration, which is possible by simply scanning through the input reviews for their occurence. In particular, it is interesting to investigate \"tight\", \"maps\" and \"rent\". The code in the cell below is a utility function to scan through the reviews for a single word:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def scan_for_word(word):\n",
      "    reviews_with_word = []\n",
      "    for review in reviews:\n",
      "        if review.find(word) != -1:\n",
      "            reviews_with_word.append(review)\n",
      "    return np.array(reviews_with_word)\n",
      "\n",
      "print 'tight\\n' + '*'*80\n",
      "print scan_for_word(\" tight \")[1]\n",
      "print scan_for_word(\" tight \")[3]\n",
      "print scan_for_word(\" tight \")[7]\n",
      "\n",
      "print '\\nmaps\\n' + '*'*80\n",
      "print scan_for_word(\" maps \")[4]\n",
      "print scan_for_word(\" maps \")[5]\n",
      "print scan_for_word(\" maps \")[6]\n",
      "\n",
      "print '\\nrent\\n' + '*'*80\n",
      "print scan_for_word(\" rent \")[11]\n",
      "print scan_for_word(\" rent \")[0]\n",
      "print scan_for_word(\" rent \")[7]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "tight\n",
        "********************************************************************************\n",
        " Burnout is one of the most intense and entertaining racing games on the PlayStation 2, complete with tight controls and impressive graphics.\n",
        " No-frills realism and tight teamwork make Insurgency a welcome break from the status quo."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " Super Mario 3D Land eases you in with creative level design and tight controls while leaving plenty of challenging content to be discovered.\n",
        "\n",
        "maps\n",
        "********************************************************************************\n",
        " Island Thunder offers more than enough in the way of a challenging single-player game and new multiplayer maps for players who are serious about tactical shooters."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " The first booster pack for Battlefield 2142 offers up three exciting new maps and a new gameplay mode, making it a must-have for fans of the multiplayer action game.\n",
        " The nine new maps are outstanding, making one of the best-ever multiplayer shooters even better."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "rent\n",
        "********************************************************************************\n",
        " The unfortunates who decide to rent or buy this game will be stuck with nothing more than a filthy, horribly buggy port of an already middling game.\n",
        " Buy Asteroids Hyper 64 if you really, really adore the classic Asteroids, but all others are advised to either rent it or forget about it entirely."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " Parents whose kids have Scooby-Doo hot on the brain from the movie can safely rent this one, or not, without worry of missing much.\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So, apparently \"tight\" is among the most positive words because it's a common term used to compliment video game design (as opposed to \"tight\" in the [UrbanDictionary](http://www.urbandictionary.com/define.php?term=Tight&defid=116155) sense of the word). Also, \"maps\" are apparently a common feature of games to be complimented in reviews. Last, we can \"rent\" is such a negative indicator because suggesting that readers rent a game is effectively an insult to the game.\n",
      "\n",
      "Finally, we can verify that, as in the case of movie reviews, our video game review classifier is also ineffective at identifying negation. For instance, we can easily construct a review that is clearly negative, yet is misclassified because the word \"not\" has been used for negation of a positive word:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf_opt.predict_proba(vectorizer_opt.transform(['This game is not excellent and could definitely use some refinements.']))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 21,
       "text": [
        "array([[ 0.01258504,  0.98741496]])"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So this review, which most humans would classify as having negative sentiment, is badly misclassified by our Naive Bayes classifier (98.7% probability of being) which fails to recognize the negation of \"excellent\" and the use of \"refinements\" which is referring to hypothetical/suggested refinements, rather than praising refinements which were implemented/included."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##8. Random Forests"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here we apply random forests to the classification problem at hand. Note that for the sake of reducing computational cost, we have set `min_df`=0.001, as larger values allow for so many features that our local machine struggles due to RAM limitations. Note that this restriction means that only the most frequent 2057 words are used as features, which is a factor of $\\sim$3.5 less features than used in the Naive Bayes classifier. The script below uses ten-fold cross-validation on the training sample to explore the change in accuracy score as a function of the number of trees in the forest:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print open('tune_random_forest.py').read()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "#!/usr/bin/env python\n",
        "# written by Aaron Meisner, 12/6/2014\n",
        "\n",
        "import cPickle\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import sklearn.grid_search\n",
        "from load_reviews_scores import load_reviews_scores\n",
        "\n",
        "def tune_random_forest(stem=False):\n",
        "    min_df_opt = 0.001 # from Naive Bayes cross-validation\n",
        "\n",
        "    # set up the list of trial n_trees values\n",
        "    n_trees = np.array([1, 2, 3, 4, 5, 10, 20, 30, 40, 50, 100, 150])\n",
        "\n",
        "    # load review/score data (pass stem keyword along)\n",
        "    reviews, scores = load_reviews_scores(stem=stem)\n",
        "\n",
        "    # construct target vector\n",
        "    Y = (scores >= 7)\n",
        "\n",
        "    # load train/test indices\n",
        "    ind_train = cPickle.load(file('data/ind_train.pkl'))\n",
        "\n",
        "    # make array of candidate min_df values\n",
        "    vectorizer = CountVectorizer(min_df=min_df_opt)\n",
        "    vectorizer.fit(reviews)\n",
        "    X = vectorizer.transform(reviews)\n",
        "\n",
        "    print X.shape\n",
        "\n",
        "    parameters = {'n_estimators' : n_trees}\n",
        "\n",
        "    clf_gs = sklearn.grid_search.GridSearchCV(RandomForestClassifier(), \n",
        "                                              parameters, cv=10)\n",
        "    clf_gs.fit(X[ind_train,:].toarray(), Y[ind_train])\n",
        "    \n",
        "    cv_means = np.array([clf_gs.grid_scores_[j][1] for j in range(len(n_trees))])\n",
        "    cv_stds = np.array([np.std(clf_gs.grid_scores_[j][2]) for j in range(len(n_trees))])\n",
        "\n",
        "    # write out results to pkl files\n",
        "    o_ntree = open('data/rf_ntrees.pkl', 'wb')\n",
        "    cPickle.dump(n_trees, o_ntree)\n",
        "    o_ntree.close()\n",
        "\n",
        "    o_score = open('data/rf_scores.pkl', 'wb')\n",
        "    cPickle.dump(cv_means, o_score)\n",
        "    o_score.close()\n",
        "\n",
        "    o_std_score = open('data/rf_std_scores.pkl', 'wb')\n",
        "    cPickle.dump(cv_stds, o_std_score)\n",
        "    o_std_score.close()\n",
        "\n",
        "\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We tested random forests with $n_{trees}$=[1, 2, 3, 4, 5, 10, 20, 30, 40, 50, 100, 150]. Below the cross-validation accuracy scores and their standard deviations are plotted as a function of $n_{trees}$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n_trees = cPickle.load(file('data/rf_ntrees.pkl'))\n",
      "rf_scores = cPickle.load(file('data/rf_scores.pkl'))\n",
      "rf_score_stds = cPickle.load(file('data/rf_std_scores.pkl'))\n",
      "\n",
      "plt.errorbar(n_trees, rf_scores, yerr=rf_score_stds)\n",
      "plt.xlabel('number of trees')\n",
      "plt.ylabel('cross-validation accuracy score')\n",
      "plt.title('tuning number of trees in random forest')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 24,
       "text": [
        "<matplotlib.text.Text at 0x1070993d0>"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEZCAYAAABrUHmEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nz4OMftPDEAAAAABJRU5ErkJggg==\n",
       "text": [
        "<matplotlib.figure.Figure at 0x108d166d0>"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##9. Predicted Ratings: K Nearest Neighbors Regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 9.1 The Regression Baseline"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the regression problem, the standard choice of metric for \"goodness\" of a model is the RMS error (RMSE) of the predictions relative to the truth. In this case, the simplest possible model simply takes the mean of the training data, and guesses that value for all of the test examples. In the following cell we have evaluated this model and calculated its RMSE:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'The mean of the training scores is : ', np.mean(scores[ind_train])\n",
      "print 'The RMSE of guessing the training data mean for every test example is', np.std(scores[ind_test]-np.mean(scores[ind_train]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The mean of the training scores is :  6.70872\n",
        "The RMSE of guessing the training data mean for every test example is 1.62154353936\n"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 9.2 KNN with Bag of Words"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 9.3 KNN using Dimensionality Reduction"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "probs_train = (clf_opt.predict_proba(X_opt[ind_train,:]))[:,1] # probability of review being positive\n",
      "scores_train = scores[ind_train]\n",
      "\n",
      "ind_sort_probs = np.argsort(probs_train)\n",
      "probs_train_sorted = probs_train[ind_sort_probs]\n",
      "scores_train_sorted = scores_train[ind_sort_probs]\n",
      "\n",
      "def my_knn_1d(review, probs_train_sorted, scores_train_sorted, median=False, n_neighbors=10):\n",
      "    prob_test = (clf_opt.predict_proba(vectorizer_opt.transform([review])))[0,1]\n",
      "    diff = np.abs(probs_train_sorted - prob_test)\n",
      "    sind_diff = np.argsort(diff)\n",
      "    \n",
      "    neighbor_scores = scores_train_sorted[sind_diff[0:(n_neighbors)]]\n",
      "    \n",
      "    result = (np.median(neighbor_scores) if median else np.mean(neighbor_scores))\n",
      "    return result\n",
      "\n",
      "def try_k_value(n_neighbors):\n",
      "    scores_pred_test = np.zeros(len(ind_test))\n",
      "    for i in range(len(ind_test)):\n",
      "        scores_pred_test[i] = my_knn_1d((reviews[ind_test])[i], probs_train_sorted,\n",
      "                                        scores_train_sorted, n_neighbors=n_neighbors)\n",
      "    return scores_pred_test\n",
      "\n",
      "def try_many_k(n_neighbors_list):\n",
      "    for n_neighbors_val in n_neighbors_list:\n",
      "        scores_pred_test = try_k_value(n_neighbors_val)\n",
      "        # print the RMS error\n",
      "        print n_neighbors_val, np.std(scores[ind_test]-scores_pred_test)\n",
      "        \n",
      "try_many_k([1,2,3,4,5,10, 20, 40, 80, 160, 320, 640, 1000, 1500, 2000, 3000])\n",
      "#plt.scatter(scores[ind_test],scores_pred_test,s=2,edgecolor='none')\n",
      "#plt.xlabel('scores for test examples')\n",
      "#plt.ylabel('scores for training examples')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1 1.71428630308\n",
        "2"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.52683078296\n",
        "3"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.45961692977\n",
        "4"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.41418189954\n",
        "5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.39567409913\n",
        "10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.34935778591\n",
        "20"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.31742412593\n",
        "40"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.30425733832\n",
        "80"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.3005675974\n",
        "160"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.29918827568\n",
        "320"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.29555519257\n",
        "640"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.29614717541\n",
        "1000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.29836050008\n",
        "1500"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.3030240617\n",
        "2000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.30916658584\n",
        "3000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.32737343028\n"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##10. Cleaning Features with Regular Expressions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer_regexp = CountVectorizer(min_df=0.00013, token_pattern=ur'\\b[a-zA-Z][a-zA-Z]+\\b')\n",
      "vectorizer_regexp.fit(reviews)\n",
      "X_regexp = vectorizer_regexp.transform(reviews)\n",
      "words_no_regexp = set(vectorizer_opt.get_feature_names())\n",
      "words_regexp = set(vectorizer_regexp.get_feature_names())\n",
      "print words_no_regexp.difference(words_regexp)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "set([u'd\\xe9j\\xe0', u'pi\\xf1ata', u'133', u'x3', u'ps2', u'ps3', u'5500', u'24', u'25', u'21st', u'20', u'21', u'22', u'1942', u'2150', u'graw2', u'400', u'2015', u'2033', u'2017', u'2011', u'2010', u'2013', u'2012', u'1993', u'121', u'f355', u'dkc2', u'15', u'1997', u'16', u'51', u'50', u'2d', u'2142', u'2k', u'2k5', u'2k6', u'989', u'2k1', u'2k2', u'2k3', u'2k8', u'2k9', u'039', u'7th', u'80', u'4x4', u'90s', u'06', u'07', u'04', u'08', u'09', u'19th', u'360', u'3d', u'n64', u'2600', u'300', u'100', u'101', u'v8so', u'2100', u'vx8000', u'sk8land', u'32', u'30', u'35', u'2049', u'v4', u'514', u'30th', u'3ds', u'clich\\xe9d', u'2nd', u'60', u'2k7', u'64', u'3000', u'66', u'clich\\xe9s', u'1987', u'2k14', u'2k15', u'2k12', u'2k13', u'2k10', u'2k11', u'1080', u'1989', u'm1', u'99', u'98', u'x8', u'2014', u'x2', u'90', u'95', u'97', u'96', u'11', u'10', u'13', u'12', u'1995', u'14', u'17', u'1996', u'19', u'18', u'10th', u'2night', u'151', u'f1', u'1980s', u'1981', u'fury3', u're4', u'233', u'40', u'ex2', u'80s', u'sk8er', u'200', u'145', u'76', u'v2', u'000', u'007', u'2009', u'3rd', u'4x', u'pok\\xe9mon', u'2084', u'2002', u'2003', u'2000', u'2001', u'2006', u'2007', u'2004', u'2005', u'2008', u'800'])\n"
       ]
      }
     ],
     "prompt_number": 47
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf_regexp = MultinomialNB(alpha=1.7).fit(X_regexp[ind_train,:], Y[ind_train])\n",
      "\n",
      "print '*'*80\n",
      "print 'NO REGEXP FILTERING'\n",
      "print \"Training set accuracy: %0.2f%%\" % (100 * clf_opt.score(X_opt[ind_train,:], Y[ind_train]))\n",
      "print \"Test set accuracy: %0.2f%%\" % (100 * clf_opt.score(X_opt[ind_test,:], Y[ind_test]))\n",
      "\n",
      "print '*'*80\n",
      "print 'WITH REGEXP FILTERING'\n",
      "print \"Training set accuracy: %0.2f%%\" % (100 * clf_regexp.score(X_regexp[ind_train,:], Y[ind_train]))\n",
      "print \"Test set accuracy: %0.2f%%\" % (100 * clf_regexp.score(X_regexp[ind_test,:], Y[ind_test]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "********************************************************************************\n",
        "NO REGEXP FILTERING\n",
        "Training set accuracy: 86.13%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Test set accuracy: 75.93%\n",
        "********************************************************************************\n",
        "WITH REGEXP FILTERING\n",
        "Training set accuracy: 85.94%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Test set accuracy: 75.98%\n"
       ]
      }
     ],
     "prompt_number": 57
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##11. Merging Features with Stemming"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "reviews_stemmed, scores = load_reviews_scores(stem=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "11803\n"
       ]
      }
     ],
     "prompt_number": 59
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer_stemmed = CountVectorizer(min_df=0.00013)\n",
      "vectorizer_stemmed.fit(reviews_stemmed)\n",
      "X_stemmed = vectorizer_stemmed.transform(reviews_stemmed)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 63
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print X_stemmed.shape\n",
      "print X_opt.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(11803, 5789)\n",
        "(11803, 7764)\n"
       ]
      }
     ],
     "prompt_number": 64
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf_stemmed = MultinomialNB(alpha=1.7).fit(X_stemmed[ind_train,:], Y[ind_train])\n",
      "print 'WITH FEATURE STEMMING'\n",
      "print \"Training set accuracy: %0.2f%%\" % (100 * clf_stemmed.score(X_stemmed[ind_train,:], Y[ind_train]))\n",
      "print \"Test set accuracy: %0.2f%%\" % (100 * clf_stemmed.score(X_stemmed[ind_test,:], Y[ind_test]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "WITH FEATURE STEMMING\n",
        "Training set accuracy: 84.25%\n",
        "Test set accuracy: 76.26%\n"
       ]
      }
     ],
     "prompt_number": 66
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##12. Visualizing the Classification Results"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 13. Full-Circle: Classifying Giant Bomb Review"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##14. Conclusions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}