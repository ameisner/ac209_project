{
 "metadata": {
  "name": "",
  "signature": "sha256:6d00560e0fb670116db183f889f77c57c8866e1ca4b39b11299f23ee925b2cbd"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Classification Process Book"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from scipy.stats import scoreatpercentile\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.cross_validation import train_test_split\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "import sklearn.grid_search\n",
      "from sklearn.ensemble import RandomForestClassifier"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#1. The Problem\n",
      "\n",
      "We wish to perform text classification and regression on GameSpot video game reviews. The classification will take the form of labeling each review as either \"positive\" or \"negative\" based on its numerical score and training/optimizing various classifiers to predict \"positive\" versus \"negative\" sentiment based on review text. This classification goal was inspired by the application of Naive Bayes classification to Rotten Tomatoes movie reviews (\"fresh\" versus \"rotten\") in the Bayesian Tomatoes exercise from AC209 Fall 2013. Here extend this prior work in several ways: (1) we apply Naive Bayes classification to video game reviews from GameSpot rather than movie reviews from Rotten Tomatoes (2) we apply feature-cleaning techniques such as regular expressions and stemming to the video game review corpus (3) we apply random forest classification and compare its results to those of Naive Bayes classification (4) we attempt to predict numerical review scores using k nearest neighbors regression."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#2. Preliminaries"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The first step to running any text analysis on the reviews is to load them in to arrays from the JSON file we wrote while scraping.\n",
      "To do this, we have written a Python utility script called `load_reviews_scores.py`. This script offers the option of either stemming or not stemming the words in the reviews as an additional preprocessing step, via the `stem` keyword in the `load_reviews_scores` function. The following code cell prints the contents of this file:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print open('load_reviews_scores.py').read()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "# written by Aaron Meisner, 12/4/2014\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "def remove_empty_reviews(rev_list, scores):\n",
        "    l = np.array([len(r.replace(' ', '')) for r in rev_list])\n",
        "\n",
        "    return rev_list[l > 0], scores[l > 0]\n",
        "\n",
        "def load_reviews_scores(stem=False):\n",
        "    f = open('data/gamespot.json')\n",
        "    data = json.load(f)\n",
        "\n",
        "    rev_list = np.array([g['summary'] for g in data])\n",
        "    scores = np.array([g['score'] for g in data]).astype(float)\n",
        "\n",
        "    rev_list, scores = remove_empty_reviews(rev_list, scores)\n",
        "\n",
        "    u, ind_u = np.unique(rev_list, return_index=True)\n",
        "    rev_list = rev_list[ind_u]\n",
        "    scores = scores[ind_u]\n",
        "\n",
        "    if stem:\n",
        "        stemmer = PorterStemmer()\n",
        "        rev_list_stemmed = []\n",
        "        tokenizer = RegexpTokenizer(ur'\\b[a-zA-Z][a-zA-Z]+\\b')\n",
        "\n",
        "        for rev in rev_list:\n",
        "            tokens = tokenizer.tokenize(rev)\n",
        "            stemmed = \"\"\n",
        "            for token in tokens:\n",
        "                stemmed = stemmed + \" \" + stemmer.stem(token)\n",
        "            rev_list_stemmed.append(stemmed)\n",
        "        return rev_list_stemmed, scores\n",
        "    else:\n",
        "        return rev_list, scores\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##3. The Baseline"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In any machine learning task, especially classification, it is extremely important to establish a baseline accuracy for the *simplest possible model*. In our case, the simplest possible model corresponds to guessing that all reviews are positive or all reviews are negative, without any examination of the review text. If we choose our threshold for \"positive\" versus \"negative\" incorrectly, we will end up with an unbalanced set of examples for which it is difficult to interpret accuracy scores. For example, in the classic fraud detection problem, 99.99% of examples could be non-fraudulently, and therefore a trivial solution can produce 99.99% nominal accuracy, depending on the accuracy metric used. In order to avoid the subtleties introduced by unbalanced examples, we will choose the boundary between \"positive\" and \"negative\" reviews which makes the examples maximally balanced. Below is code showing how we choose the boundary between positive and negative reviews."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from load_reviews_scores import load_reviews_scores\n",
      "reviews, scores = load_reviews_scores()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'The median review score is ', scoreatpercentile(scores, 50)\n",
      "print 'The fraction of reviews that have score greater than or equal to 7 is', float(np.sum(scores >=  7))/len(scores)\n",
      "print 'The fraction of reviews that have score greater than 7 is', float(np.sum(scores > 7))/len(scores)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The median review score is  7.0\n",
        "The fraction of reviews that have score greater than or equal to 7 is 0.535880708295\n",
        "The fraction of reviews that have score greater than 7 is 0.461747013471\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can see that we can get closest to an even split of the data by labeling \"positive\" reviews as those with score greater than\n",
      "or equal to seven, and \"negative\" reviews as those with score less than seven. With this choice of threshold, a trivial solution can achieve accuracy of 53.6% at best, by guessing that every review is positive."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##4. Train/Test Split"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the Bayesian Tomatoes exercise, 3000 reviews were collected in total. Here, we have nearly 12000 reviews. Thus, we have plenty of data and can choose a substantial portion to be set aside completely "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'The total number of GameSpot reviews that we have is', len(reviews)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The total number of GameSpot reviews that we have is 11803\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Throughout our machine learning work, we will set aside aside 1803 of our examples as a test set, leaving 10000 examples on which to train and cross-validate."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In order to perform our train/test split, we need to set up our feature matrix $X$ and target vector $Y$. As in Bayesian Tomatoes, we take features to be individual words by using the \"bag of words\" technique. In the following cell we set up the feature matrix $X$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer = CountVectorizer()\n",
      "vectorizer.fit(reviews)\n",
      "X = vectorizer.transform(reviews)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Per the discussion above, the target vector $Y$ will simply be a vector of 1's and 0's and is extremely easy to calculate. We will make the labels such that 1 corresponds to a \"positive\" review, and \"0\" corresponds to a \"negative review\""
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Y = (scores >= 7)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can check that X and Y have compatible dimensions:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print X.shape\n",
      "print Y.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(11803, 13631)\n",
        "(11803,)\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note that the second dimension of $X$ encodes the number of features. With this default vectorizer, we actually have more \n",
      "features than examples! The number of features can be adjusted with the `df_min` parameter of CountVectorizer (we will do this later)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we can actually perform the train/test split:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=10000)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##5. Naivest Bayes"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here we try the simplest possible version of Naive Bayes. As demonstrated in Bayesian Tomatoes, there are two important parameters which must be tuned. The first is `min_df`, which encodes the minimum frequency with which a word must appear in the training corpus in order to become a feature of the model. This is useful in getting rid of random junk. In Bayesian tomatoes, it was found that `min_df`=0.001 is optimal, but here we simply use the default value. The other important parameter that should be tuned is `alpha`, the additive (Laplace/Lidstone) smoothing parameter, which effectively smooths out individual features and thereby dictates how strongly the predicted positive/negative probabilities are dragged toward 0.50. Here, to establish a baseline, we adopt the default value of `alpha`=1."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf = MultinomialNB().fit(X_train, Y_train)\n",
      "print \"Training set accuracy: %0.2f%%\" % (100 * clf.score(X_train, Y_train))\n",
      "print \"Test set accuracy: %0.2f%%\" % (100 * clf.score(X_test, Y_test))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training set accuracy: 89.39%\n",
        "Test set accuracy: 74.49%\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Thus, without being careful about cleaning or stemming our features, and without any cross-validation to tune the model parameters, we can already achieve an accuracy of $\\sim$75% on the test data. That's pretty good!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##6. Cross-Validation and Calibration Framework"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In general we will use the GridSearchCV method to perform ten-fold cross validation. We will also produce a set of calibration plots similar to those of Bayesian Tomatoes part 3.4."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##7. Tuned Naive Bayes Classifier"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "alphas = [0.005,.1, 0.5, 1, 2, 5, 10]\n",
      "parameters = {'alpha' : alphas}\n",
      "clf_gs = sklearn.grid_search.GridSearchCV(MultinomialNB(), parameters, cv=10)\n",
      "clf_gs.fit(X_train, Y_train)\n",
      "clf_gs.grid_scores_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "[mean: 0.71710, std: 0.00933, params: {'alpha': 0.005},\n",
        " mean: 0.74110, std: 0.01072, params: {'alpha': 0.1},\n",
        " mean: 0.75540, std: 0.01138, params: {'alpha': 0.5},\n",
        " mean: 0.75960, std: 0.01029, params: {'alpha': 1},\n",
        " mean: 0.75740, std: 0.01123, params: {'alpha': 2},\n",
        " mean: 0.73480, std: 0.01391, params: {'alpha': 5},\n",
        " mean: 0.69590, std: 0.01064, params: {'alpha': 10}]"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_min_df(min_df):\n",
      "    vectorizer_ = CountVectorizer(min_df=min_df)\n",
      "    vectorizer_.fit(reviews)\n",
      "    X_ = vectorizer_.transform(reviews)\n",
      "    X_train_, X_test_, Y_train_, Y_test_ = train_test_split(X_, Y, train_size=10000)\n",
      "    clf_ = MultinomialNB().fit(X_train_, Y_train_)\n",
      "    print X_.shape\n",
      "    print clf_.score(X_test_, Y_test_)\n",
      "    \n",
      "test_min_df(0.00001)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(11803, 13631)\n",
        "0.751525235718\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##8. Random Forests"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer_opt = CountVectorizer(min_df=0.001)\n",
      "vectorizer_opt.fit(reviews)\n",
      "print len(vectorizer_opt.get_feature_names())\n",
      "\n",
      "X_opt = vectorizer_opt.transform(reviews)\n",
      "\n",
      "X_opt.shape\n",
      "\n",
      "X_opt_train, X_opt_test, Y_opt_train, Y_opt_test = train_test_split(X_opt, Y, train_size=10000)\n",
      "\n",
      "X_opt_train.shape\n",
      "n_tree = 1200\n",
      "rfc = RandomForestClassifier(n_estimators=n_tree)\n",
      "rfc.fit(X_opt_train.toarray(), Y_opt_train)\n",
      "rfc.score(X_opt_test.toarray(), Y_opt_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2057\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 39,
       "text": [
        "0.74653355518580145"
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##9. Predicted Ratings: K Nearest Neighbors Regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##10. Cleaning Features with Regular Expressions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##11. Merging Features with Stemming"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}